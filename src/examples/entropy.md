# Shannon Entropy

## Equation

$$
\mark[entropy]{H} = - \mark[average]{\sum_{i}} \mark[prob]{p(x_i)} \mark[bits]{\log_2 p(x_i)}
$$

## Description

[Entropy]{.entropy} is the [average]{.average} [information content]{.bits} generated by a source, weighted by the [probability]{.prob} of each outcome. It measures "surprise" or "uncertainty".

## .entropy

Shannon Entropy ($H$).

Quantifies the uncertainty in a system. It represents the theoretical minimum average number of bits needed to encode a message from this source. High entropy = pure randomness (maximum surprise). Low entropy = predictability.

## .average

Expected Value ($-\sum$).

We calculate the weighted average over all possible outcomes. The negative sign is necessary because $\log(p)$ is negative for probabilities $p < 1$; it ensures the resulting entropy is positive.

## .prob

Probability ($p(x_i)$).

The likelihood of a specific event occurring. Common events have high probability (and low information). Rare events have low probability (and high information).

## .bits

Log-probability ($\log_2 p$).

Measures the "length" or information content of a specific event in bits. An event with $p=1/2$ provides 1 bit of information ($-\log_2 0.5 = 1$). An event with $p=1/8$ provides 3 bits. We learn more when a rare event happens.

